{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fd7293-5f2c-4d35-84d3-a4feb87c4614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2023-12-26 21:28:26,553\", \"levelname\": \"INFO\", \"name\": \"root\", \"message\": \"Loading secret ADLS_adlsthuehomelakehousedev_access_key\"}\n",
      ":: loading settings :: url = jar:file:/opt/bitnami/python/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.3-bundle_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-76a1b8b2-ebf9-4ed5-a4b0-c87caa1bcd2f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 1364ms :: artifacts dl 57ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.3 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.hudi#hudi-spark3.3-bundle_2.12;0.13.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   27  |   0   |   0   |   0   ||   27  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-76a1b8b2-ebf9-4ed5-a4b0-c87caa1bcd2f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 27 already retrieved (0kB/32ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/26 21:28:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "from cdk.services.api.energi_data_service import EnergiDataService\n",
    "from cdk.common_modules.access.secrets import Secrets\n",
    "from cdk.common_modules.spark.spark_config import SparkConfig\n",
    "from cdk.common_modules.spark.spark_session_builder import SparkSessionBuilder\n",
    "from cdk.common_modules.utility.dwh_columns import DwhColumns \n",
    "\n",
    "storage_account_name = \"adlsthuehomelakehousedev\"\n",
    "\n",
    "# Set Spark configurations\n",
    "spark_config = SparkConfig(Secrets())\n",
    "\n",
    "# Add jars to install\n",
    "spark_config.add_jars_to_install(['hudi', 'azure_storage', 'delta'])\n",
    "\n",
    "# Add storage account access\n",
    "spark_config.add_storage_account_access(storage_account_name, method='access_key')\n",
    "\n",
    "# Build SparkSession\n",
    "spark = SparkSessionBuilder(\"ViewAzureData\", spark_config).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781facba-5fc0-4c8d-83fc-806e303c7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"stock_transactions\"\n",
    "dataset_path = f\"nordea\"#/20231210/{dataset_name}\"\n",
    "container_name = \"landing\"\n",
    "destination_container_name = \"bronze\"\n",
    "\n",
    "# landing_path = f\"hdfs://namenode:9000/data/landing/energi_data_service/{dataset_name}\"\n",
    "landing_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{dataset_path}\"\n",
    "bronze_path = f\"abfss://{destination_container_name}@{storage_account_name}.dfs.core.windows.net/delta/{dataset_path}/{dataset_name}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ca242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2023-12-16 13:38:45,728\", \"levelname\": \"INFO\", \"name\": \"cdk.common_modules.utility.logging\", \"message\": \"Running add_filename with args: (<cdk.common_modules.utility.dwh_columns.DwhColumns object at 0x7f52f481c640>, DataFrame[Status: string, Instrument navn: string, Opdateret den: string, Transaktionstype: string, Antal: string, Total: string, _c6: string, year: int, month: int, day: int]) and kwargs: {}\"}\n",
      "{\"asctime\": \"2023-12-16 13:38:45,783\", \"levelname\": \"INFO\", \"name\": \"cdk.common_modules.utility.logging\", \"message\": \"Finished running add_filename\"}\n",
      "{\"asctime\": \"2023-12-16 13:38:45,786\", \"levelname\": \"INFO\", \"name\": \"cdk.common_modules.utility.logging\", \"message\": \"Running add_updated_timestamp with args: (<cdk.common_modules.utility.dwh_columns.DwhColumns object at 0x7f52f481c640>, DataFrame[Status: string, Instrument navn: string, Opdateret den: string, Transaktionstype: string, Antal: string, Total: string, _c6: string, year: int, month: int, day: int]) and kwargs: {}\"}\n",
      "{\"asctime\": \"2023-12-16 13:38:45,801\", \"levelname\": \"INFO\", \"name\": \"cdk.common_modules.utility.logging\", \"message\": \"Finished running add_updated_timestamp\"}\n",
      "+------+-------------------------------+-------------+----------------+-----+------------+----+----+-----+---+---------------------------------------------------------------------+\n",
      "|Status|Instrument navn                |Opdateret den|Transaktionstype|Antal|Total       |_c6 |year|month|day|dwh_filename                                                         |\n",
      "+------+-------------------------------+-------------+----------------+-----+------------+----+----+-----+---+---------------------------------------------------------------------+\n",
      "|Udført|A/s Jyske Bank                 |06-12-2023   |Udbytte         |0.0  |101,14 DKK  |null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|TORM plc A Aktie               |05-12-2023   |Udbytte         |0.0  |279,72 DKK  |null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Meta Platforms Inc A           |05-12-2023   |Sælg            |2.0  |4.373,11 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Microsoft Corp                 |04-12-2023   |Sælg            |2.0  |5.142,21 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|D/S Norden                     |28-11-2023   |Køb             |13.0 |4.272,20 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Royal UNIBREW                  |27-11-2023   |Køb             |10.0 |4.633,00 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|IONQ Inc                       |27-11-2023   |Køb             |2.0  |174,84 DKK  |null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|IONQ Inc                       |27-11-2023   |Køb             |67.0 |5.857,65 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Qualcomm Inc                   |27-11-2023   |Køb             |7.0  |6.194,00 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|TORM A                         |24-11-2023   |Sælg            |28.0 |6.058,20 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Dampskibsselskabet 'Norden' A/S|07-11-2023   |Udbytte         |0.0  |150,00 DKK  |null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Bavarian Nordic                |03-11-2023   |Køb             |29.0 |3.922,25 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Ambu                           |17-10-2023   |Køb             |71.0 |4.976,28 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|ISS                            |17-10-2023   |Køb             |37.0 |3.989,85 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Mercadolibre Inc               |17-10-2023   |Køb             |1.0  |8.726,00 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Royal UNIBREW                  |17-10-2023   |Køb             |6.0  |3.109,40 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|D/S Norden                     |17-10-2023   |Køb             |15.0 |6.467,00 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|TORM A                         |17-10-2023   |Sælg            |17.0 |3.394,80 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|Jyske Bank                     |06-10-2023   |Køb             |13.0 |6.399,00 DKK|null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "|Udført|TORM plc A Aktie               |12-09-2023   |Udbytte         |0.0  |470,25 DKK  |null|2023|12   |10 |nordea/stock_transactions/year=2023/month=12/day=10/Transaktioner.csv|\n",
      "+------+-------------------------------+-------------+----------------+-----+------------+----+----+-----+---+---------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# from cdk.common_modules.utility import dwh_columns\n",
    "from cdk.common_modules.utility.dwh_columns import DwhColumns \n",
    "\n",
    "df = (\n",
    "    spark.read.csv(landing_path, header=True, sep=\";\")\n",
    "    # .withColumn(\"date\", F.to_date(F.concat_ws('-','year','month','day'), \"yyyy-MM-dd\"))\n",
    "    # .filter((F.col(\"date\")>datetime.date(2023,12,9)))\n",
    ")\n",
    "\n",
    "# df.show()\n",
    "\n",
    "dwh_columns = DwhColumns()\n",
    "\n",
    "df_filename = dwh_columns.add_filename(df)\n",
    "\n",
    "df_timestamp =dwh_columns.add_updated_timestamp(df)\n",
    "\n",
    "df_filename.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021e1ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Schema' from 'pyspark.sql' (/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame, Schema\n\u001b[1;32m      3\u001b[0m d \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtoday()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(d\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Schema' from 'pyspark.sql' (/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "d = datetime.datetime.today()\n",
    "\n",
    "print(d.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6b3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def test(self,a: int,b: int):\n",
    "        return a+b\n",
    "    \n",
    "tst = Test()\n",
    "\n",
    "test = getattr(tst, \"test\")\n",
    "\n",
    "def transform_data(df: int) -> int:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c375861",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not infer schema from empty dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/opt/bitnami/python/lib/python3.9/site-packages/pyspark/sql/session.py:514\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03mInfer schema from list of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m:class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema from empty dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[1;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n",
      "\u001b[0;31mValueError\u001b[0m: can not infer schema from empty dataset"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e3e3e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "try:\n",
    "    df_temp = (\n",
    "        spark.read.format(\"delta\") \n",
    "            .option(\"readChangeFeed\", \"true\") \n",
    "            .option(\"startingTimestamp\", '2023-12-17 05:45:46') \n",
    "            .load(bronze_path)\n",
    "    )\n",
    "    df_temp.show()\n",
    "except AnalysisException as e:\n",
    "    if \"Please use a timestamp before or at\" in str(e):\n",
    "        print(\"No new data\")\n",
    "    else:\n",
    "        raise e\n",
    "    # df_temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1671cfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7ff8917a9cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "bronze_path_tmp = f\"abfss://{destination_container_name}@{storage_account_name}.dfs.core.windows.net/delta/{dataset_path}/test_stock_transactions\"\n",
    "df = (\n",
    "    spark.read.format('delta').load(bronze_path)\n",
    "    # .withColumn(\"date\", F.to_date(F.concat_ws('-','year','month','day'), \"yyyy-MM-dd\"))\n",
    "    # .filter((F.col(\"date\")>datetime.date(2023,12,9)))\n",
    ")\n",
    "# df.show()\n",
    "(\n",
    "    DeltaTable\n",
    "        .createIfNotExists(spark)\n",
    "        # .tableName(file.name)\n",
    "        .addColumns(df.schema)\n",
    "        .location(bronze_path_tmp)\n",
    "        .property(\"delta.enableChangeDataFeed\", \"true\").execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30bbf16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('append')\n",
    "        .save(bronze_path_tmp)\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
