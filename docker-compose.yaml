version: "3.6"
volumes:
#   shared-workspace:
#     name:  "notebooks" #"hadoop-distributed-file-system"
#     driver: local
  hdfs-config:
  # name-storage:
services:
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
      - 9010:9000
    environment:
      - CLUSTER_NAME=test
      # - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - "ENSURE_NAMENODE_DIR=/tmp/hadoop-root/dfs/name"
    volumes:
      - hdfs-config:/opt/hadoop/etc/hadoop
    env_file:
      - ./hadoop.env
    networks:
      - my_persistent_network
  datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      # CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    # volumes:
    #   - data-storage:/tmp/hadoop-hadoop/dfs
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode
    networks:
      - my_persistent_network    
  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    networks:
      - my_persistent_network
  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    networks:
      - my_persistent_network

  jupyterlab:
    image: jupyterlab
    container_name: jupyterlab
    ports:
      - 8888:8888
    volumes:
      - ./src/jupyter:/opt/workspace
    networks:
      - my_persistent_network 
    depends_on:
      - spark-master
  spark-master:
    image: spark-master
    container_name: spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - 8089:8080
      - 7077:7077
    # volumes:
    #   - shared-workspace:/opt/workspace
    depends_on:
      - namenode
      - datanode
    networks:
      - my_persistent_network  
  spark-worker-1:
    image: spark-worker
    container_name: spark-worker-1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8081:8081
    # volumes:
    #   - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
    networks:
      - my_persistent_network  
  spark-worker-2:
    image: spark-worker
    container_name: spark-worker-2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8082:8081
    # volumes:
    #   - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
    networks:
      - my_persistent_network  

  postgresql:
    image: docker.io/bitnami/postgresql:15
    volumes:
      - './data/postgresql_data:/bitnami/postgresql'
    environment:
      - POSTGRESQL_DATABASE=bitnami_airflow
      - POSTGRESQL_USERNAME=bn_airflow
      - POSTGRESQL_PASSWORD=bitnami1
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '5432:5432'
    networks:
      - my_persistent_network 
  redis:
    image: docker.io/bitnami/redis:7.0
    volumes:
      - './data/redis_data:/bitnami'
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    networks:
      - my_persistent_network 
  airflow-scheduler:
    image: docker.io/bitnami/airflow-scheduler:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
    networks:
      - my_persistent_network 
  airflow-worker:
    image: docker.io/bitnami/airflow-worker:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
    networks:
      - my_persistent_network 
  airflow:
    image: docker.io/bitnami/airflow:2
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_PASSWORD=bitnami
      - AIRFLOW_USERNAME=user
      - AIRFLOW_EMAIL=user@example.com
    volumes:
      - './data/airflow_data:/opt/bitnami/airflow/dags'
    ports:
      - '8080:8080' 
    networks:
      - my_persistent_network 

  # configuration manager for NiFi
  zookeeper:
    hostname: zookeeper
    container_name: zookeeper_container
    image: 'bitnami/zookeeper:3.9.0'  # latest image as of 2021-11-09.
    restart: on-failure
    environment:
        - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
        - my_persistent_network
# version control for nifi flows
  registry:
    hostname: myregistry
    container_name: registry_container_persistent
    image: 'apache/nifi-registry:1.23.2'
    restart: on-failure
    ports:
        - "18080:18080"
    environment:
        - LOG_LEVEL=INFO
        - NIFI_REGISTRY_DB_DIR=/opt/nifi-registry/nifi-registry-current/database
        - NIFI_REGISTRY_FLOW_PROVIDER=git #file
        - NIFI_REGISTRY_FLOW_STORAGE_DIR=/usr/src #/opt/nifi-registry/nifi-registry-current/flow_storage
    volumes:
        # - ./nifi_registry/database:/opt/nifi-registry/nifi-registry-current/database
        # - ./nifi_registry/flow_storage:/opt/nifi-registry/nifi-registry-current/flow_storage
        - ./src/nifi:/usr/src
        # uncomment the next line after copying the /conf directory from the container to your local directory to persist NiFi flows
        # Run docker cp <Container ID>:/opt/nifi-registry/nifi-registry-current/conf ./nifi_registry/
        # - ./providers.xml:/opt/nifi-registry/nifi-registry-current/conf/providers.xml
    networks:
        - my_persistent_network
# data extraction, transformation and load service
  nifi:
    hostname: nifi
    container_name: nifi_container
    image: 'apache/nifi:1.23.2' 
    restart: on-failure
    ports:
        - '8091:8080'
    environment:
        - NIFI_WEB_HTTP_PORT=8080
        - NIFI_CLUSTER_IS_NODE=true
        - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
        - NIFI_ZK_CONNECT_STRING=zookeeper:2181
        - NIFI_ELECTION_MAX_WAIT=30 sec
        - NIFI_SENSITIVE_PROPS_KEY='12345678901234567890A'
    healthcheck:
        test: "${DOCKER_HEALTHCHECK_TEST:-curl localhost:8091/nifi/}"
        interval: "60s"
        timeout: "3s"
        start_period: "5s"
        retries: 5
    volumes:
        # - ./nifi/database_repository:/opt/nifi/nifi-current/database_repository
        # - ./nifi/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
        # - ./nifi/content_repository:/opt/nifi/nifi-current/content_repository
        # - ./nifi/provenance_repository:/opt/nifi/nifi-current/provenance_repository
        # - ./nifi/state:/opt/nifi/nifi-current/state
        # - ./nifi/logs:/opt/nifi/nifi-current/logs
        - ./data/in:/opt/nifi/datatest/in
        - ./data/out:/opt/nifi/datatest/out
        - hdfs-config:/opt/hadoop/etc/hadoop
        # uncomment the next line after copying the /conf directory from the container to your local directory to persist NiFi flows
        # docker cp <Container ID>:/opt/nifi/nifi-current/conf ./nifi/
        # - ./nifi/conf:/opt/nifi/nifi-current/conf
    networks:
        - my_persistent_network
    depends_on:
        - zookeeper
        - registry
        - namenode
  azurite:
    image: mcr.microsoft.com/azure-storage/azurite
    command: "azurite --loose --blobHost 0.0.0.0 --blobPort 10000 --queueHost 0.0.0.0 --queuePort 10001 --location /workspace --debug /workspace/debug.log"
    ports:
      - 10000:10000
      - 10001:10001
      - 10002:10002
    volumes:
      - ./azurite:/workspace
    networks:
      - my_persistent_network
networks:
  my_persistent_network:
    driver: bridge