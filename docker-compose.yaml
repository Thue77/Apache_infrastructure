version: "3.6"
volumes:
  broker: null
  esdata: null
  mysqldata: null
  zkdata: null
services:
  ad-hoc:
    build:
      context: ./
      dockerfile: docker/ad-hoc/Dockerfile
    # image: jupyterlab
    container_name: ad-hoc
    ports:
      - 8888:8888
    environment:
      - PYTHONPATH=/opt/workspace/apps:/opt/workspace/scripts:/opt/workspace/dags
      - ADLS_adlsthuehomelakehousedev_access_key=$data_lake_access_key
    volumes:
      - ./src/ad-hoc:/opt/workspace
      - ./src/airflow_dags:/opt/workspace/dags
      - ./settings/jupyter:/root/.vscode-server
    networks:
      - my_persistent_network 
    depends_on:
      - spark-master
  spark-master:
    build:
      context: ./
      dockerfile: docker/spark-master/Dockerfile
    # image: spark-master
    container_name: spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - 8089:8080
      - 7077:7077
    networks:
      - my_persistent_network  
  spark-worker-1:
    build:
      context: ./
      dockerfile: docker/spark-worker/Dockerfile
    # image: spark-worker
    container_name: spark-worker-1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 8082:8081
    # volumes:
    #   - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
    networks:
      - my_persistent_network  

  postgresql:
    # image: docker.io/bitnami/postgresql:15
    build:
      context: ./
      dockerfile: docker/postgresql/Dockerfile
    volumes:
      - ./data/postgresql_data:/bitnami/postgresql/data
    environment:
      - POSTGRESQL_DATABASE=bitnami_airflow
      - POSTGRESQL_USERNAME=bn_airflow
      - POSTGRESQL_PASSWORD=bitnami1
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    ports:
      - '5432:5432'
    networks:
      - my_persistent_network 
  redis:
    # image: docker.io/bitnami/redis:7.0
    build:
      context: ./
      dockerfile: docker/redis/Dockerfile
    volumes:
      - './data/redis_data:/bitnami'
    environment:
      # ALLOW_EMPTY_PASSWORD is recommended only for development.
      - ALLOW_EMPTY_PASSWORD=yes
    networks:
      - my_persistent_network 
  airflow-scheduler:
    build:
      context: ./
      dockerfile: docker/airflow-scheduler/Dockerfile
    # image: airflow-scheduler
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow
      - AIRFLOW_LOAD_EXAMPLES=no
      # - PYTHONPATH=/opt/bitnami/airflow/dags:/opt/bitnami/airflow/plugins:/opt/bitnami/python/scripts
    volumes:
      - ./src/airflow_dags:/opt/bitnami/airflow/dags
      - ./src/jupyter/apps:/opt/bitnami/spark/apps
      # - ./src/jupyter/scripts:/opt/bitnami/python/scripts
      # - ./dependencies/requirements.txt:/bitnami/python/requirements.txt
    networks:
      - my_persistent_network 
  airflow-worker:
    build:
      context: ./
      dockerfile: docker/airflow-worker/Dockerfile
    # image: airflow-worker
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_WEBSERVER_HOST=airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow
      - AIRFLOW_LOAD_EXAMPLES=no
      - PYTHONPATH=/opt/bitnami/airflow/dags:/opt/bitnami/airflow/plugins:/opt/bitnami/python/scripts
      - ADLS_adlsthuehomelakehousedev_access_key=$data_lake_access_key
    volumes:
      - ./src/airflow_dags:/opt/bitnami/airflow/dags
      - ./src/ad-hoc/apps:/opt/bitnami/spark/apps
      - ./src/ad-hoc/scripts:/opt/bitnami/python/scripts
      # - ./dependencies/requirements.txt:/bitnami/python/requirements.txt
    networks:
      - my_persistent_network 
  airflow:
    build:
      context: ./
      dockerfile: docker/airflow/Dockerfile
    # image: airflow
    environment:
      - AIRFLOW_DATABASE_NAME=bitnami_airflow
      - AIRFLOW_DATABASE_USERNAME=bn_airflow
      - AIRFLOW_DATABASE_PASSWORD=bitnami1
      - AIRFLOW_EXECUTOR=CeleryExecutor
      - AIRFLOW_PASSWORD=bitnami
      - AIRFLOW_USERNAME=user
      - AIRFLOW_EMAIL=user@example.com
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow
      - AIRFLOW_LOAD_EXAMPLES=no
      # - PYTHONPATH=/opt/bitnami/airflow/dags:/opt/bitnami/airflow/plugins:/opt/bitnami/python/scripts
      # - AIRFLOW_HOME=/opt/bitnami/airflow
    volumes:
      - ./src/airflow_dags:/opt/bitnami/airflow/dags
      - ./settings/airflow:/.vscode-server
      # - ./src/jupyter/apps:/opt/bitnami/spark/apps
      # - ./src/jupyter/scripts:/opt/bitnami/python/scripts
      # - ./dependencies/requirements.txt:/bitnami/python/requirements.txt
    ports:
      - '9080:8080' 
    networks:
      - my_persistent_network 
  
  trino-coordinator:
    extends:
      file: docker-compose-analytics.yaml
      service: trino-coordinator
    networks:
    - my_persistent_network  

  superset:
    extends:
      file: docker-compose-analytics.yaml
      service: superset
    networks:
    - my_persistent_network  

  metastore_db:
    extends:
      file: docker-compose-analytics.yaml
      service: metastore_db
    networks:
    - my_persistent_network  

  hive-metastore:
    extends:
      file: docker-compose-analytics.yaml
      service: hive-metastore
    networks:
    - my_persistent_network  
  

  # broker:
  #   container_name: broker
  #   depends_on:
  #     zookeeper:
  #       condition: service_healthy
  #   environment:
  #   - KAFKA_BROKER_ID=1
  #   - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
  #   - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #   - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
  #   - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
  #   - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
  #   - KAFKA_HEAP_OPTS=-Xms256m -Xmx256m
  #   - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
  #   healthcheck:
  #     interval: 1s
  #     retries: 5
  #     start_period: 60s
  #     test: nc -z broker $${DATAHUB_MAPPED_KAFKA_BROKER_PORT:-9092}
  #     timeout: 5s
  #   hostname: broker
  #   image: confluentinc/cp-kafka:7.4.0
  #   ports:
  #   - ${DATAHUB_MAPPED_KAFKA_BROKER_PORT:-9092}:9092
  #   volumes:
  #   - broker:/var/lib/kafka/data/
  #   networks:
  #     - my_persistent_network
  # datahub-actions:
  #   container_name: datahub-actions
  #   depends_on:
  #     datahub-gms:
  #       condition: service_healthy
  #   environment:
  #   - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
  #   - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
  #   - DATAHUB_GMS_HOST=datahub-gms
  #   - DATAHUB_GMS_PORT=8084
  #   - DATAHUB_GMS_PROTOCOL=http
  #   - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
  #   - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
  #   - KAFKA_BOOTSTRAP_SERVER=broker:29092
  #   - KAFKA_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT
  #   - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
  #   - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
  #   - SCHEMA_REGISTRY_URL=http://schema-registry:8081
  #   hostname: actions
  #   image: ${DATAHUB_ACTIONS_IMAGE:-acryldata/datahub-actions}:${ACTIONS_VERSION:-head}
  #   networks:
  #     - my_persistent_network
  # datahub-frontend-react:
  #   container_name: datahub-frontend-react
  #   depends_on:
  #     datahub-gms:
  #       condition: service_healthy
  #   environment:
  #   - DATAHUB_GMS_HOST=datahub-gms
  #   - DATAHUB_GMS_PORT=8084
  #   - DATAHUB_SECRET=YouKnowNothing
  #   - DATAHUB_APP_VERSION=1.0
  #   - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
  #   - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
  #   - KAFKA_BOOTSTRAP_SERVER=broker:29092
  #   - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
  #   - ELASTIC_CLIENT_HOST=elasticsearch
  #   - ELASTIC_CLIENT_PORT=9200
  #   hostname: datahub-frontend-react
  #   image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
  #   ports:
  #   - ${DATAHUB_MAPPED_FRONTEND_PORT:-9002}:9002
  #   volumes:
  #   - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  #   networks:
  #     - my_persistent_network
  # datahub-gms:
  #   container_name: datahub-gms
  #   depends_on:
  #     datahub-upgrade:
  #       condition: service_completed_successfully
  #   environment:
  #   - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}
  #   - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
  #   - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
  #   - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
  #   - EBEAN_DATASOURCE_HOST=mysql:3306
  #   - EBEAN_DATASOURCE_PASSWORD=datahub
  #   - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
  #   - EBEAN_DATASOURCE_USERNAME=datahub
  #   - ELASTICSEARCH_HOST=elasticsearch
  #   - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
  #   - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
  #   - ELASTICSEARCH_PORT=9200
  #   - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
  #   - ENTITY_SERVICE_ENABLE_RETENTION=true
  #   - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
  #   - GRAPH_SERVICE_DIFF_MODE_ENABLED=true
  #   - GRAPH_SERVICE_IMPL=elasticsearch
  #   - JAVA_OPTS=-Xms1g -Xmx1g
  #   - KAFKA_BOOTSTRAP_SERVER=broker:29092
  #   - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
  #   - MAE_CONSUMER_ENABLED=true
  #   - MCE_CONSUMER_ENABLED=true
  #   - PE_CONSUMER_ENABLED=true
  #   - UI_INGESTION_ENABLED=true
  #   healthcheck:
  #     interval: 1s
  #     retries: 3
  #     start_period: 90s
  #     test: curl -sS --fail http://datahub-gms:${DATAHUB_MAPPED_GMS_PORT:-8084}/health
  #     timeout: 5s
  #   hostname: datahub-gms
  #   image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
  #   ports:
  #   - ${DATAHUB_MAPPED_GMS_PORT:-8084}:8080
  #   volumes:
  #   - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  #   networks:
  #     - my_persistent_network
  # datahub-upgrade:
  #   command:
  #   - -u
  #   - SystemUpdate
  #   container_name: datahub-upgrade
  #   depends_on:
  #     elasticsearch-setup:
  #       condition: service_completed_successfully
  #     kafka-setup:
  #       condition: service_completed_successfully
  #     mysql-setup:
  #       condition: service_completed_successfully
  #   environment:
  #   - EBEAN_DATASOURCE_USERNAME=datahub
  #   - EBEAN_DATASOURCE_PASSWORD=datahub
  #   - EBEAN_DATASOURCE_HOST=mysql:3306
  #   - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
  #   - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
  #   - KAFKA_BOOTSTRAP_SERVER=broker:29092
  #   - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
  #   - ELASTICSEARCH_HOST=elasticsearch
  #   - ELASTICSEARCH_PORT=9200
  #   - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
  #   - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
  #   - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
  #   - GRAPH_SERVICE_IMPL=elasticsearch
  #   - DATAHUB_GMS_HOST=datahub-gms
  #   - DATAHUB_GMS_PORT=8084
  #   - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
  #   - BACKFILL_BROWSE_PATHS_V2=true
  #   hostname: datahub-upgrade
  #   image: ${DATAHUB_UPGRADE_IMAGE:-acryldata/datahub-upgrade}:${DATAHUB_VERSION:-head}
  #   labels:
  #     datahub_setup_job: true
  #   networks:
  #     - my_persistent_network
  # elasticsearch:
  #   container_name: elasticsearch
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 1G
  #   environment:
  #   - discovery.type=single-node
  #   - ${XPACK_SECURITY_ENABLED:-xpack.security.enabled=false}
  #   - ES_JAVA_OPTS=-Xms256m -Xmx512m -Dlog4j2.formatMsgNoLookups=true
  #   - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m -Dlog4j2.formatMsgNoLookups=true
  #   healthcheck:
  #     interval: 1s
  #     retries: 3
  #     start_period: 20s
  #     test: curl -sS --fail http://elasticsearch:$${DATAHUB_MAPPED_ELASTIC_PORT:-9200}/_cluster/health?wait_for_status=yellow&timeout=0s
  #     timeout: 5s
  #   hostname: elasticsearch
  #   image: ${DATAHUB_SEARCH_IMAGE:-elasticsearch}:${DATAHUB_SEARCH_TAG:-7.10.1}
  #   ports:
  #   - ${DATAHUB_MAPPED_ELASTIC_PORT:-9200}:9200
  #   volumes:
  #   - esdata:/usr/share/elasticsearch/data
  #   networks:
  #     - my_persistent_network
  # elasticsearch-setup:
  #   container_name: elasticsearch-setup
  #   depends_on:
  #     elasticsearch:
  #       condition: service_healthy
  #   environment:
  #   - ELASTICSEARCH_USE_SSL=${ELASTICSEARCH_USE_SSL:-false}
  #   - USE_AWS_ELASTICSEARCH=${USE_AWS_ELASTICSEARCH:-false}
  #   - ELASTICSEARCH_HOST=elasticsearch
  #   - ELASTICSEARCH_PORT=9200
  #   - ELASTICSEARCH_PROTOCOL=http
  #   hostname: elasticsearch-setup
  #   image: ${DATAHUB_ELASTIC_SETUP_IMAGE:-linkedin/datahub-elasticsearch-setup}:${DATAHUB_VERSION:-head}
  #   labels:
  #     datahub_setup_job: true
  #   networks:
  #     - my_persistent_network
  # kafka-setup:
  #   container_name: kafka-setup
  #   depends_on:
  #     broker:
  #       condition: service_healthy
  #     schema-registry:
  #       condition: service_healthy
  #   environment:
  #   - DATAHUB_PRECREATE_TOPICS=${DATAHUB_PRECREATE_TOPICS:-false}
  #   - KAFKA_BOOTSTRAP_SERVER=broker:29092
  #   - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
  #   - USE_CONFLUENT_SCHEMA_REGISTRY=TRUE
  #   hostname: kafka-setup
  #   image: ${DATAHUB_KAFKA_SETUP_IMAGE:-linkedin/datahub-kafka-setup}:${DATAHUB_VERSION:-head}
  #   labels:
  #     datahub_setup_job: true
  #   networks:
  #     - my_persistent_network
  # mysql:
  #   command: --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --default-authentication-plugin=mysql_native_password
  #   container_name: mysql
  #   environment:
  #   - MYSQL_DATABASE=datahub
  #   - MYSQL_USER=datahub
  #   - MYSQL_PASSWORD=datahub
  #   - MYSQL_ROOT_PASSWORD=datahub
  #   healthcheck:
  #     interval: 1s
  #     retries: 5
  #     start_period: 20s
  #     test: mysqladmin ping -h mysql -u $$MYSQL_USER --password=$$MYSQL_PASSWORD
  #     timeout: 5s
  #   hostname: mysql
  #   image: mysql:5.7
  #   ports:
  #   - ${DATAHUB_MAPPED_MYSQL_PORT:-3306}:3306
  #   restart: on-failure
  #   volumes:
  #   - ../mysql/init.sql:/docker-entrypoint-initdb.d/init.sql
  #   - mysqldata:/var/lib/mysql
  #   networks:
  #     - my_persistent_network
  # mysql-setup:
  #   container_name: mysql-setup
  #   depends_on:
  #     mysql:
  #       condition: service_healthy
  #   environment:
  #   - MYSQL_HOST=mysql
  #   - MYSQL_PORT=3306
  #   - MYSQL_USERNAME=datahub
  #   - MYSQL_PASSWORD=datahub
  #   - DATAHUB_DB_NAME=datahub
  #   hostname: mysql-setup
  #   image: ${DATAHUB_MYSQL_SETUP_IMAGE:-acryldata/datahub-mysql-setup}:${DATAHUB_VERSION:-head}
  #   labels:
  #     datahub_setup_job: true
  #   networks:
  #     - my_persistent_network
  # schema-registry:
  #   container_name: schema-registry
  #   depends_on:
  #     broker:
  #       condition: service_healthy
  #   environment:
  #   - SCHEMA_REGISTRY_HOST_NAME=schemaregistry
  #   - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=PLAINTEXT
  #   - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092
  #   healthcheck:
  #     interval: 1s
  #     retries: 3
  #     start_period: 60s
  #     test: nc -z schema-registry ${DATAHUB_MAPPED_SCHEMA_REGISTRY_PORT:-8081}
  #     timeout: 5s
  #   hostname: schema-registry
  #   image: confluentinc/cp-schema-registry:7.4.0
  #   ports:
  #   - ${DATAHUB_MAPPED_SCHEMA_REGISTRY_PORT:-8081}:8081
  #   networks:
  #     - my_persistent_network
  # zookeeper:
  #   container_name: zookeeper
  #   environment:
  #   - ZOOKEEPER_CLIENT_PORT=2181
  #   - ZOOKEEPER_TICK_TIME=2000
  #   healthcheck:
  #     interval: 5s
  #     retries: 3
  #     start_period: 30s
  #     test: echo srvr | nc zookeeper $${DATAHUB_MAPPED_ZK_PORT:-2181}
  #     timeout: 5s
  #   hostname: zookeeper
  #   image: confluentinc/cp-zookeeper:7.4.0
  #   ports:
  #   - ${DATAHUB_MAPPED_ZK_PORT:-2181}:2181
  #   volumes:
  #   - zkdata:/var/lib/zookeeper
  #   networks:
  #     - my_persistent_network

networks:
  my_persistent_network:
    driver: bridge